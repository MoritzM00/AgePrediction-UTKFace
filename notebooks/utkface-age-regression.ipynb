{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint, WandbEvalCallback\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W&B Setup\n",
    "\n",
    "Go to `Add-ons` -> `Secrets` and add your API Key here with name `WANDB_API_KEY`. Select the checkbox under `Attach to Notebook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.create(\n",
    "    dict(\n",
    "        data_path=\"/kaggle/input/utkface-cropped/UTKFace/\",\n",
    "        img_size=(200, 200),\n",
    "        target_size=(224, 224),\n",
    "        n_channels=3,\n",
    "        wandb_project=\"UTKFace-Age-Regression\",\n",
    "        wandb_group=\"EfficientNet\",\n",
    "        models_dir=\"models\",\n",
    "        use_sample_weight=False,\n",
    "        use_tensorboard=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = OmegaConf.create(\n",
    "    dict(\n",
    "        architecture=\"EfficientNetV2B0\",\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        lr_schedule=\"ExponentialDecay\",\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        loss=\"mean_absolute_error\",\n",
    "        optimizer=\"Adam\",\n",
    "        early_stopping_patience=5,\n",
    "        early_stopping_monitor=\"val_mae\",\n",
    "        early_stopping_mode=\"min\",\n",
    "        random_translation=0.1,\n",
    "        random_rotation=0.1,\n",
    "        random_flip=\"horizontal\",\n",
    "        resize_and_rescale=False,  # not needed for efficient net\n",
    "        augment=False,\n",
    "        augment_gpu=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the training and testing dataset using the recommended tf.data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.list_files(cfg.data_path + \"*\")\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    # read the age from the filename\n",
    "    filename = tf.strings.split(file_path, os.sep)[-1]\n",
    "    label = tf.strings.split(filename, \"_\")[0]\n",
    "    label = tf.strings.to_number(label, out_type=tf.dtypes.int32)\n",
    "\n",
    "    # read and decode the image\n",
    "    raw = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(raw, channels=cfg.n_channels)\n",
    "    print(\"Initial shape: \", image.shape)\n",
    "    image = tf.image.resize(image, [*cfg.target_size])\n",
    "    image.set_shape([*cfg.target_size, cfg.n_channels])\n",
    "    print(\"Final shape: \", image.shape)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "labeled_dataset = dataset.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in labeled_dataset.take(1):\n",
    "    print(\"Image shape: \", img.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for i, (image, label) in enumerate(labeled_dataset.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().astype(\"int32\"))\n",
    "    plt.title(int(label))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ds: tf.data.Dataset, split: float = 0.8):\n",
    "    train_size = int(len(ds) * 0.8)\n",
    "    test_size = len(ds) - train_size\n",
    "\n",
    "    train_ds = labeled_dataset.shuffle(1000).take(train_size)\n",
    "    test_ds = labeled_dataset.skip(train_size).take(test_size)\n",
    "    print(f\"Train size: {train_size}\")\n",
    "    print(f\"Test size: {test_size}\")\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "train_ds, test_ds = train_test_split(labeled_dataset, split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([Resizing(64, 64), Rescaling(1.0 / 255)])\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        RandomRotation(factor=model_cfg.random_rotation),\n",
    "        RandomTranslation(\n",
    "            width_factor=model_cfg.random_translation,\n",
    "            height_factor=model_cfg.random_translation,\n",
    "        ),\n",
    "        RandomFlip(mode=model_cfg.random_flip),\n",
    "        # RandomBrightness(factor=0.2)\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config):\n",
    "    inputs = keras.Input(shape=(*cfg.target_size, cfg.n_channels))\n",
    "    base_model = keras.applications.EfficientNetV2B0(\n",
    "        include_top=False,\n",
    "        input_tensor=inputs,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # training=False is very important if we unfreeze the base_model later on\n",
    "    # for an explanation of the difference between training=False in the call function and the trainable attribute,\n",
    "    # see here https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute\n",
    "    if config.augment_gpu:\n",
    "        x = data_augmentation(inputs)  # do it on GPU\n",
    "    else:\n",
    "        x = inputs\n",
    "    x = base_model(x, training=False)\n",
    "\n",
    "    # Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"relu\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=config.initial_learning_rate,\n",
    "        decay_steps=config.decay_steps,\n",
    "        decay_rate=config.decay_rate,\n",
    "    )\n",
    "    metrics = [\"mae\"]\n",
    "    weighted_metrics = (\n",
    "        [keras.metrics.MeanAbsoluteError(name=\"mae_weighted\")]\n",
    "        if cfg.use_sample_weight\n",
    "        else None\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=config.loss,\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        metrics=metrics,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "    )\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=model_cfg.early_stopping_monitor,\n",
    "    verbose=1,\n",
    "    patience=model_cfg.early_stopping_patience,\n",
    "    mode=model_cfg.early_stopping_mode,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement model prediction visualization callback\n",
    "class WandbClfEvalCallback(WandbEvalCallback):\n",
    "    \"\"\"Classification Evaluation Callback that logs predictions to Weights and biases.\n",
    "\n",
    "    This Callback runs after each epoch and logs a single batch of predictions\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, validation_data, data_table_columns, pred_table_columns, n_samples=8\n",
    "    ):\n",
    "        super().__init__(data_table_columns, pred_table_columns)\n",
    "\n",
    "        self.data = validation_data\n",
    "\n",
    "        if n_samples > model_cfg.batch_size:\n",
    "            raise ValueError(\"n_samples must be smaller than batch size.\")\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def add_ground_truth(self, logs=None):\n",
    "        # TODO: sample weight support\n",
    "        for images, labels in self.data.take(1).as_numpy_iterator():\n",
    "            for idx, (img, label) in enumerate(zip(images, labels)):\n",
    "                self.data_table.add_data(idx, wandb.Image(img), label)\n",
    "                if idx == self.n_samples - 1:\n",
    "                    return\n",
    "\n",
    "    def add_model_predictions(self, epoch, logs=None):\n",
    "        preds = self.model.predict(self.data.take(1), verbose=0)\n",
    "\n",
    "        table_idxs = self.data_table_ref.get_index()\n",
    "\n",
    "        for idx in table_idxs:\n",
    "            pred = preds[idx][0]\n",
    "            self.pred_table.add_data(\n",
    "                epoch,\n",
    "                self.data_table_ref.data[idx][0],\n",
    "                self.data_table_ref.data[idx][1],\n",
    "                self.data_table_ref.data[idx][2],\n",
    "                pred,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(image, 0), training=True)\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
    "        plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def prepare(ds: tf.data.Dataset, shuffle=False, augment=False, resize_and_rescale=True):\n",
    "    if resize_and_rescale:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # Batch all datasets.\n",
    "    ds = ds.batch(model_cfg.batch_size)\n",
    "\n",
    "    # Use data augmentation only on the training set.\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare(\n",
    "    train_ds,\n",
    "    shuffle=True,\n",
    "    augment=model_cfg.augment,\n",
    "    resize_and_rescale=model_cfg.resize_and_rescale,\n",
    ")\n",
    "test_ds = prepare(test_ds, resize_and_rescale=model_cfg.resize_and_rescale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(run_id: str, version: int):\n",
    "    \"\"\"Restores the model from the run with the given id and version (does not equal epoch in general).\n",
    "\n",
    "    Downloads the artifact from W&B and returns the model. Use this, if the kernel crashed.\n",
    "    Otherwise you can use `wandb.restore(name=<model-name>)`\n",
    "    \"\"\"\n",
    "    model_name = f\"run_{run_id}_model:v{version}\"\n",
    "    artifact_name = f\"moritzm00/UTKFace-Age-Regression/{model_name}\"\n",
    "    if wandb.run is not None:\n",
    "        artifact = run.use_artifact(artifact_name, type=\"model\")\n",
    "    else:\n",
    "        api = wandb.Api()\n",
    "        artifact = api.artifact(artifact_name, type=\"model\")\n",
    "    artifact_dir = artifact.download()\n",
    "    model = tf.keras.models.load_model(f\"/kaggle/working/artifacts/{model_name}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = (\n",
    "    wandb.util.generate_id()\n",
    ")  # use this to resume a run, (also set resume=\"must\" to be sure it is resuming)\n",
    "print(\"Run id is:\", run_id)\n",
    "resume = \"allow\"\n",
    "run = wandb.init(\n",
    "    id=run_id,\n",
    "    project=cfg.wandb_project,\n",
    "    group=cfg.wandb_group,\n",
    "    config=OmegaConf.to_object(model_cfg),\n",
    "    resume=resume,\n",
    "    sync_tensorboard=cfg.use_tensorboard,\n",
    "    tags=[\"EfficientNetV2B0\", \"Tensorboard\"],\n",
    "    notes=\"EfficientNetV2B0 Model with tensorboard logging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, base_model = build_model(model_cfg)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.use_tensorboard:\n",
    "    callbacks = [\n",
    "        early_stopping,\n",
    "        WandbMetricsLogger(),\n",
    "        WandbModelCheckpoint(\n",
    "            cfg.models_dir + \"/model-{epoch:02d}-{val_mae:.2f}\",\n",
    "            monitor=model_cfg.early_stopping_monitor,\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        WandbClfEvalCallback(\n",
    "            validation_data=test_ds,\n",
    "            data_table_columns=[\"idx\", \"image\", \"label\"],\n",
    "            pred_table_columns=[\"epoch\", \"idx\", \"image\", \"label\", \"pred\"],\n",
    "            n_samples=8,\n",
    "        ),\n",
    "    ]\n",
    "else:\n",
    "    callbacks = [early_stopping, TensorBoard(log_dir=\"./logs\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=model_cfg.epochs,\n",
    "    validation_data=test_ds,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "\n",
    "base_model.trainable = True\n",
    "# or just some layers:\n",
    "# for layer in model.layers[:-40]:\n",
    "#     layer.trainable = True\n",
    "    \n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-5, # low learning rate\n",
    "    decay_steps=model_cfg.decay_steps,\n",
    "    decay_rate=0.99,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr_schedule), \n",
    "    loss=model_cfg.loss,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "print(model.summary())\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=run.step + model_cfg.epochs,\n",
    "    initial_epoch=run.step,\n",
    "    validation_data=test_ds,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for _, label in labeled_dataset.as_numpy_iterator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(labeled_dataset.batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(preds, bins=50, label=\"predictions\")\n",
    "plt.hist(labels, bins=50, label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"label\": labels, \"pred\": preds.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"label\").agg({\"pred\": \"mean\"}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
